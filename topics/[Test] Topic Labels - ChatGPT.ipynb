{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8462596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/climatechange-ai-tutorials/nlp-policy-analysis/blob/main/paris_prompts.ipynb#scrollTo=9jVjv8etAIiQ\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# this will signify to the model the start of the input text\n",
    "INPUT_MARKER = 'Input Text'\n",
    "\n",
    "# This dictionary just contains the names of the columns to slightly longer names that are easier to understand.\n",
    "column_names = {'goal': 'Sustainable Development Goal', 'target': 'Subgoal', 'status': 'Status', 'sector': 'Sector', 'response': 'Climate Response', 'infotype': 'Information Type'}\n",
    "\n",
    "SPECIAL_TOKEN = \"**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will skip the special_token as well as the what the \"context\" column is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_prompt(row, pred_column, obs_column, test_input_only=False):\n",
    "    ''' Args:\n",
    "        row (pd.Series): a row from the dataframe\n",
    "        obs_column (str): the column that we are observing (e.g. text, context)\n",
    "        test_input_only (bool):\n",
    "            - if True, only return the input text\n",
    "                - This is used for the test example.\n",
    "            - if False, return the input text and the correct answer\n",
    "                - This will be for in-context learning\n",
    "        Returns:\n",
    "            prompt (str): the second portion of the prompt\n",
    "    '''\n",
    "    prompt = f'\\n{INPUT_MARKER}: {row[obs_column]}\\n'\n",
    "    if test_input_only:\n",
    "        return prompt\n",
    "\n",
    "    value = row[pred_column]\n",
    "    prompt += f'{pred_column}:{value}\\n'\n",
    "    prompt += '\\n'\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# sample a random row from the dataframe to use as a test example\n",
    "random_test_row = df_full.sample(1).iloc[0]\n",
    "test_input = build_single_prompt(random_test_row, pred_column='goal', obs_column='text', test_input_only=True)\n",
    "\n",
    "prompt_example += test_input\n",
    "prompt_example += f'{\"goal\"}:'\n",
    "# here we have the full instruction and prompt for the model\n",
    "print(prompt_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24320b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# To use the model, you will need to sign up for an OpenAI Account (http://platform.openai.com/signup) and get an API key pasted below.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"INSERT_YOUR_KEY_HERE\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# so we will use this particular model\n",
    "MODEL_NAME = \"text-davinci-003\"\n",
    "\n",
    "def get_model_response(test_prompt, pay=False):\n",
    "  ''' test_prompt (str)\n",
    "        - The full text prompt to send to the API for completion.\n",
    "      pay (bool):\n",
    "        - If False, return a dummy response. If True, send the prompt to the model API\n",
    "        - This simple flag to let us test the function without calling the API which costs money.\n",
    "      Returns:\n",
    "        response (dict): the response from the API or a dummy response if Pay is False\n",
    "  '''\n",
    "  if not pay:\n",
    "    # this is a dummy response that always returns the class 3\n",
    "    response = {'choices':[{'text':'3'}]}\n",
    "  else:\n",
    "    response = openai.Completion.create(\n",
    "      model=MODEL_NAME,\n",
    "      prompt=test_prompt,\n",
    "      temperature=0,\n",
    "      max_tokens=4,\n",
    "      top_p=1.0,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0\n",
    "    )\n",
    "  return response\n",
    "\n",
    "\n",
    "# For now we can just test the function, but you can see what happens by setting pay=True\n",
    "response = get_model_response(prompt_example, pay=False)\n",
    "example_pred = response['choices'][0]['text']\n",
    "print(example_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
